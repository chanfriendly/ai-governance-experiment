# Daily Research Journal: 2025-03-02

## Current Phase
- Phase: [1]
- Focus Area: Testing the model performance before adding agent framework.

## Activities Completed
1. Ran Deepseek R1 1.5b and 8b with Oumi.
2. Tried to install quantized model on M1 Mac.
3. Installed llama.cpp and [Distilled DeepSeek R1 8b](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF)

## Observations
### Key Findings
- Framework not ready to move forward to models. Have not successfully prompted model from Oumi yet.
- Non-Nvidia hardware causing roadblocks. Tried to build Ollama > Oumi bridge, but just added more complexity and still errored.

## Decisions Made
- Decision: Pause and research alternatives.
  - Rationale: I'd rather take a beat and find the optimal solution instead of the fast solution. If I can have a 3b+ model run in a modular system that allows for upgrades, additions, etc., it's worth asking the question instead of jumping immediately into a non-optimal solution.
  - Alternatives Considered:  Llama.cpp, Unsloth, transitioning to Jetson Orin (Nvidia product).
  - Implications: The fun doesn't start just yet. I'm excited to get to the models interacting, but I want to make sure there's value in how we get there.

## Challenges Encountered
- Quantizing 8b model on Oumi difficulties. Running into an error when running quant model, and both 8b and 1.5b models hang on prompt:
	- Enter your input prompt: what's your favorite color? 
		- 2025-03-02 10:08:53,986][oumi][rank0][pid:15985][MainThread][INFO]][native_text_inference_engine.py:140] Setting EOS token id to `151643`
  - Impact:  Have to downsize to 1.5b model until solution is found. I'd really like to, as that extra power would be amazing to have.
  - Resolution/Plan: Need to resolve and get *any* model running with Oumi right now. 8b runs on Ollama no problem. 1.5b eventually does run with Oumi, but it takes longer than the 8b does with Ollama. Second time did not run after 30 min+. It also appears there may be an issue trying to do this with a non-Nvidia GPU: 
	  - The error message indicates that the version of bitsandbytes you’re using was compiled without GPU support, which may not be compatible with the quantization you’re trying to do. Additionally, it’s still showing an issue with the 4-bit quantization due to missing support on your M1 Mac.
	  - **Why This Is Happening:**
		
		• **M1 Macs**: The M1 architecture (ARM-based) has limitations compared to traditional x86 architecture when it comes to GPU-accelerated libraries, particularly for things like bitsandbytes which depend on CUDA for GPU acceleration (which is not available for M1 Macs).
		
		• **No GPU Support**: Your error message indicates that the bitsandbytes library installed was compiled without GPU support, and since you’re on an M1 Mac (which uses Apple’s own GPU), this is leading to the incompatibility.


## Questions Raised
- How to move forward with models?
  - Context: Oumi was supposed to be the additional layer for managing agent behavior, but getting a model to run is proving difficult. Trying to incorporate a quantized/UnSloth model to be more efficient is causing issues because of the non-Nvidia GPU. Even if I just forged ahead with a 1.5-3b model with Ollama, I'd have to step back and think about the impacts and understand how Ollama would support multiple agents with one model.
  - Potential approaches to answer: Maybe the Jetson Orin will work better? It has less resources than the M1 Mac, but it being an Nvidia device might be worth it, especially if we can use UnSloth or quantized models. There's [also this article ](https://medevel.com/running-llms-on-apple-silicon-m1-11/) talking about using llama.cpp, which has popped up in research. It specifically mentions "Apple silicon is a first class citizen," which sounds promising.
- [Question 2]
  - [...]

## Next Steps
-  Research Llama.cpp and how it supports M1, and how Oumi would integrate with it.
- If it checks out, implement, test, and troubleshoot towards a promptable model in Oumi.
- As an alternate, put Oumi and a quantized model on Jetson Orin for a test environment.
- Integrate llama.cpp with Oumi. There's also a [convert tool to use quanted version ](https://github.com/ggml-org/llama.cpp/blob/master/convert_hf_to_gguf_update.py)with llama.cpp

## Resources Used
- [Papers referenced]
-  Unsloth, Hugging Face, Oumi
- [People consulted]

## Time Log
- Start time: [Time]
- End time: [Time]
- Total hours: [Hours]
- Primary activities breakdown:
  - [Activity 1]: [Hours]
  - [Activity 2]: [Hours]
  - [...]
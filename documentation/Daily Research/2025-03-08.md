# Daily Research Journal:

## Current Phase
- Phase: [2]
- Focus Area: Low level summary progress, jotting down some thoughts from the weekend.

## Activities Completed
1. Summarized and made audio versions of several Anthropic papers around AI governance and morality.
2. Read [80000hours.org](https://80000hours.org/problem-profiles/artificial-intelligence/) profile on AI. It expanded some thoughts I have about the alignment problem.

## Observations
### Key Findings
- The big question I (we?) are trying to answer is: How do we make sure AI does good for us? There are a few sub-questions that come out of this:
	- How do we define "good?" Morality in one country or culture is different than in another.
	- We're trying to solve a grey area question with a black-and-white answer; dogma. ALWAYS do X. Philosophy shows us any extreme stance leads to harm.
	- Do we want our models to always do anything, knowing the bad actors that exist today can weaponize it? Leaving gray area open means we can combat authoritarians, but also that gray area leaves room for the authoritarians to manipulate.


## Challenges Encountered
- Our existing structures are not prepared for AI, and AI is not prepared to withstand bad actors in our current structure.
  - Impact: It feels very "damned if you do " right now. Bad actors will always take the low road to win, but following them down the low road to stay ahead of them feels just as bad.
  - Resolution/Plan: Keep policy in mind during the experiment. Ideally, there would be some kind of ceasefire drawn on AI progress so we can all collaborate on safety. I feel this is an ideal way to take a breath and build trust between nations at a time of escalations.
- We are trying to solve a problem without all the tools. Before we can solve how to govern AI, we need to advance our understanding of governing. 
	- Impact: Just like DeepMind changed the game on predicting protein structures, we need to use AI to help advance what we see as "good" and how to care for each other so we can apply it to our systems. Right now, AI is built in our image, and our image is dysfunctional. I see this like accounting for human bias in our algorithms.
	- Resolution/Plan: Ask more from the community here. Understand more about how to train models.
- We are largely, if not entirely, training our models on rewards, or winning.
	- Impact: Arrival said it best: [When you're given a hammer, everything is a nail](https://www.youtube.com/watch?v=8kdWlQ__wk8). If we're training them to be the best, if everything is a game with a winner and loser, eventually we become the nail.
	- Resolution/Plan: For human sciences, is it possible to reconsider how training is done? Does another method already exist? Supervised/Unsupervised, Reinforcement, etc; do any of these support the type of result we want? Is this even possible?



## Next Steps
- I might make a full blog post about this to expand outside a notes section.
- Research if this concept is already covered, or if there's existing resources on the question(s)
- Pose the question to Oumi AI beginners channel, or Panos since he seems to be a big research resource. **Do your own research first, be sure the question needs to be asked, and make sure you're prepared if it does.**
- Deeper research on Few-Shot Learning and Multi-Agent Reinforcement Learning (MARL), which could be the training answers.
- Spend more time reading deeper into Anthropic papers.

## Resources Used
- Arrival, Denis Villeneuve
- Claude, 80000hours.org
